# HuggingFace Accelerate ì™„ì „ ê°€ì´ë“œ

## Accelerateë€?

**HuggingFace Accelerate**ëŠ” PyTorch ë¶„ì‚° í•™ìŠµì„ ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

### ê¸°ì¡´ ë°©ë²•ì˜ ë¬¸ì œì  (PyTorch Distributed)

```python
# ë³µì¡í•œ ìˆ˜ë™ ì„¤ì •ì´ í•„ìš”
import torch.distributed as dist
import os

# í™˜ê²½ ë³€ìˆ˜ ìˆ˜ë™ ì„¤ì •
os.environ['MASTER_ADDR'] = '192.168.1.100'
os.environ['MASTER_PORT'] = '29500'
os.environ['RANK'] = '0'
os.environ['WORLD_SIZE'] = '3'

# ë¶„ì‚° ì´ˆê¸°í™”
dist.init_process_group(backend='nccl')
local_rank = int(os.environ['LOCAL_RANK'])
torch.cuda.set_device(local_rank)

# ëª¨ë¸ì„ ê° GPUì— ë°°ì¹˜
model = model.to(local_rank)
model = torch.nn.parallel.DistributedDataParallel(model)
# ... ë³µì¡í•œ ì½”ë“œ ...
```

### Accelerateë¥¼ ì‚¬ìš©í•˜ë©´

```python
from accelerate import Accelerator

# ì´ê²ƒë§Œ í•˜ë©´ ë!
accelerator = Accelerator()

# ëª¨ë¸, ë°ì´í„°ë¡œë” ìë™ ì¤€ë¹„
model, optimizer, train_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader
)

# í•™ìŠµ ë£¨í”„ëŠ” ë™ì¼í•˜ê²Œ ì‘ì„±
for batch in train_dataloader:
    outputs = model(**batch)
    loss = outputs.loss
    
    # backwardë„ ìë™ ì²˜ë¦¬
    accelerator.backward(loss)
    optimizer.step()
```

## Accelerateì˜ í•µì‹¬ ì¥ì 

### 1. **ìë™ ì„¤ì •**
- GPU ê°œìˆ˜ ìë™ ê°ì§€
- ë©€í‹° ë…¸ë“œ ì„¤ì • ìë™ ì²˜ë¦¬
- ë©”ëª¨ë¦¬ ìµœì í™” ìë™ ì ìš©

### 2. **ì½”ë“œ ë³€ê²½ ìµœì†Œí™”**
- ê¸°ì¡´ í•™ìŠµ ì½”ë“œë¥¼ ê±°ì˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
- ë‹¨ì¼ GPU ì½”ë“œ â†’ ë©€í‹° GPU/ë…¸ë“œë¡œ ì‰½ê²Œ í™•ì¥

### 3. **ê°„ë‹¨í•œ ì‹¤í–‰**
```bash
# ë‹¨ì¼ GPU
python train.py

# ì—¬ëŸ¬ GPU (ê°™ì€ ì»´í“¨í„°)
accelerate launch train.py

# ì—¬ëŸ¬ ì»´í“¨í„° (3ëŒ€)
accelerate launch --multi_gpu --num_machines=3 train.py
```

## ë‹¹ì‹ ì˜ ê²½ìš°: 3ëŒ€ ì»´í“¨í„°ì— ì ìš©

### ë‹¨ê³„ 1: Accelerate ì„¤ì¹˜
```bash
pip install accelerate
```

### ë‹¨ê³„ 2: ì„¤ì • íŒŒì¼ ìƒì„± (í•œ ë²ˆë§Œ)

#### ë©”ì¸ ë…¸ë“œ (ì²« ë²ˆì§¸ ì»´í“¨í„°)ì—ì„œ:
```bash
accelerate config
```

ì§ˆë¬¸ì— ë‹µë³€:
```
- Multi-node training: **yes**
- Main node IP address: **192.168.1.100** (ë©”ì¸ ë…¸ë“œ IP)
- Main node port: **29500**
- Total number of nodes: **3**
- Current node rank: **0** (0 = ì²« ë²ˆì§¸ ë…¸ë“œ)
- Which GPU(s): **0** (ê° ì»´í“¨í„°ì—ì„œ GPU 0ë²ˆ ì‚¬ìš©)
- Mixed precision: **fp16** (ë” ë¹ ë¥¸ í•™ìŠµ)
```

ì„¤ì • íŒŒì¼ì´ `~/.cache/huggingface/accelerate/default_config.yaml`ì— ìƒì„±ë©ë‹ˆë‹¤.

#### ë‹¤ë¥¸ ë…¸ë“œì—ë„ ì„¤ì • (ê°„ë‹¨íˆ)
- ì„¤ì • íŒŒì¼ì„ ë³µì‚¬í•˜ê±°ë‚˜
- ê° ë…¸ë“œì—ì„œ `accelerate config`ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ê³  `node_rank`ë§Œ ë³€ê²½ (ë…¸ë“œ2: 1, ë…¸ë“œ3: 2)

### ë‹¨ê³„ 3: ì½”ë“œ ìˆ˜ì • (ìµœì†Œí•œ)

#### ê¸°ì¡´ ì½”ë“œ (`02_finetune_local.py`)
```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)
trainer.train()
```

#### Accelerate ì‚¬ìš© (ê±°ì˜ ë³€ê²½ ì—†ìŒ!)
```python
# TrainerëŠ” ì´ë¯¸ Accelerateë¥¼ ìë™ ì§€ì›!
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)
trainer.train()  # ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ë¶„ì‚° í•™ìŠµ!
```

### ë‹¨ê³„ 4: ì‹¤í–‰ (ë§¤ìš° ê°„ë‹¨!)

#### ëª¨ë“  ë…¸ë“œì—ì„œ ë™ì‹œì— ì‹¤í–‰:
```bash
accelerate launch 02_finetune_local.py
```

ë! ğŸ‰

## Accelerate vs ìˆ˜ë™ Distributed

| í•­ëª© | ìˆ˜ë™ Distributed | Accelerate |
|------|-----------------|------------|
| **ì„¤ì • ì½”ë“œ** | 50+ ì¤„ | 1ì¤„ |
| **í™˜ê²½ ë³€ìˆ˜** | ìˆ˜ë™ ì„¤ì • | ìë™ ì²˜ë¦¬ |
| **ì‹¤í–‰ ëª…ë ¹** | ë³µì¡í•œ launch ì˜µì…˜ | `accelerate launch` |
| **ë””ë²„ê¹…** | ì–´ë ¤ì›€ | ì‰¬ì›€ |
| **ìœ ì§€ë³´ìˆ˜** | ë³µì¡ | ê°„ë‹¨ |

## ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ë‹¨ì¼ ì»´í“¨í„°, GPU ì—¬ëŸ¬ ê°œ
```bash
accelerate launch --num_processes=4 train.py  # GPU 4ê°œ ì‚¬ìš©
```

### ì˜ˆì‹œ 2: ì—¬ëŸ¬ ì»´í“¨í„° (ë‹¹ì‹ ì˜ ê²½ìš°)
```bash
# ëª¨ë“  ì»´í“¨í„°ì—ì„œ ë™ì‹œì— ì‹¤í–‰
accelerate launch train.py
```

### ì˜ˆì‹œ 3: Trainer ì‚¬ìš© ì‹œ (ê°€ì¥ ê°„ë‹¨!)
```python
# HuggingFace TrainerëŠ” Accelerateë¥¼ ìë™ ì§€ì›!
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    ddp_find_unused_parameters=False,  # ë¶„ì‚° í•™ìŠµ ìµœì í™”
)

trainer = Trainer(...)
trainer.train()  # ê·¸ëƒ¥ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ë¶„ì‚° ì²˜ë¦¬!
```

## ë‹¹ì‹ ì˜ í”„ë¡œì íŠ¸ì— ë°”ë¡œ ì ìš©í•˜ê¸°

### ë°©ë²• A: ê¸°ì¡´ ì½”ë“œ ìœ ì§€ (Trainer ìë™ ì§€ì›)

`02_finetune_local.py`ë¥¼ ê±°ì˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©:
```bash
# ëª¨ë“  ë…¸ë“œì—ì„œ
accelerate launch 02_finetune_local.py
```

Trainerê°€ ìë™ìœ¼ë¡œ ë¶„ì‚° í•™ìŠµì„ ì²˜ë¦¬í•©ë‹ˆë‹¤!

### ë°©ë²• B: Accelerate ì§ì ‘ ì‚¬ìš© (ë” ì„¸ë°€í•œ ì œì–´)

`02_finetune_with_accelerate.py` ìƒì„±:
```python
from accelerate import Accelerator

accelerator = Accelerator()

# ëª¨ë¸, ë°ì´í„° ì¤€ë¹„
model, train_dataloader = accelerator.prepare(model, train_dataloader)

# í•™ìŠµ ë£¨í”„
for epoch in range(num_epochs):
    for batch in train_dataloader:
        # ... í•™ìŠµ ì½”ë“œ ...
        accelerator.backward(loss)
```

## ì£¼ì˜ì‚¬í•­

1. **ëª¨ë“  ë…¸ë“œì—ì„œ ë™ì‹œ ì‹¤í–‰**í•´ì•¼ í•©ë‹ˆë‹¤
2. **ê°™ì€ ë„¤íŠ¸ì›Œí¬**ì— ì—°ê²°ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
3. **ê°™ì€ í”„ë¡œì íŠ¸ í´ë”ì™€ ë°ì´í„°**ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
4. **ë°©í™”ë²½ ì„¤ì •**: í¬íŠ¸ê°€ ì—´ë ¤ ìˆì–´ì•¼ í•©ë‹ˆë‹¤

## ìš”ì•½

**Accelerate = ë¶„ì‚° í•™ìŠµì„ ì‰½ê²Œ!**

- âœ… ë³µì¡í•œ ì„¤ì • ì—†ìŒ
- âœ… ì½”ë“œ ë³€ê²½ ìµœì†Œí™”
- âœ… ìë™ ìµœì í™”
- âœ… ê°„ë‹¨í•œ ì‹¤í–‰

ë‹¹ì‹ ì˜ 3ëŒ€ ì»´í“¨í„° í™˜ê²½ì— ìµœì ì˜ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤! ğŸš€
